{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "80773cdecc613a186b9eac051636553d2ff0f99c57590601666ed713bd5d0256"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# <a id='0'>Contents</a>\n",
    "\n",
    "- <a href='#2'>Importing Packages</a>  \n",
    "- <a href='#3'>Uploading Data</a>\n",
    "- <a href='#4'>Creating Train, Validation, and Testing Sets</a>  \n",
    "- <a href='#5'>Data Cleaning</a>  \n",
    "- <a href='#5'>Exploratory Data Analysis</a>\n",
    "- <a href='#6'>Feature Engineering</a>  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Introduction\n",
    "\n",
    "This dataset contains information on default payments, demographic factors, credit data, history of payment, and bill statements of credit card customers in Taiwan from April 2005 to September 2005.\n",
    "\n",
    "\n",
    "There are 25 variables:\n",
    "\n",
    "- ID: ID of each client\n",
    "- LIMIT_BAL: Amount of given credit in NT dollars (includes individual and family/supplementary credit\n",
    "- SEX: Gender\n",
    "    - 1=male\n",
    "    - 2=female\n",
    "- EDUCATION:\n",
    "    - 1=graduate school\n",
    "    - 2=university\n",
    "    - 3=high school\n",
    "    - 4=others\n",
    "    - 5=unknown\n",
    "    - 6=unknown\n",
    "- MARRIAGE: Marital status\n",
    "    - 1=married\n",
    "    - 2=single\n",
    "    - 3=others)\n",
    "- AGE: Age in years\n",
    "- PAY_0: Repayment status in September, 2005\n",
    "    - -1=pay duly\n",
    "    - 1=payment delay for one month\n",
    "    - 2=payment delay for two months\n",
    "    ....\n",
    "    - 8=payment delay for eight months\n",
    "    - 9=payment delay for nine months and above\n",
    "- PAY_2: Repayment status in August, 2005 (scale same as above)\n",
    "- PAY_3: Repayment status in July, 2005 (scale same as above)\n",
    "- PAY_4: Repayment status in June, 2005 (scale same as above)\n",
    "- PAY_5: Repayment status in May, 2005 (scale same as above)\n",
    "- PAY_6: Repayment status in April, 2005 (scale same as above)\n",
    "- BILL_AMT1: Amount of bill statement in September,2005 (NT dollar)\n",
    "- BILL_AMT2: Amount of bill statement in August, 2005 (NT dollar)\n",
    "- BILL_AMT3: Amount of bill statement in July, 2005 (NT dollar)\n",
    "- BILL_AMT4: Amount of bill statement in June, 2005 (NT dollar)\n",
    "- BILL_AMT5: Amount of bill statement in May, 2005 (NT dollar)\n",
    "- BILL_AMT6: Amount of bill statement in April, 2005 (NT dollar)\n",
    "- PAY_AMT1: Amount of previous payment in September, 2005 (NT dollar)\n",
    "- PAY_AMT2: Amount of previous payment in August, 2005 (NT dollar)\n",
    "- PAY_AMT3: Amount of previous payment in July, 2005 (NT dollar)\n",
    "- PAY_AMT4: Amount of previous payment in June, 2005 (NT dollar)\n",
    "- PAY_AMT5: Amount of previous payment in May, 2005 (NT dollar)\n",
    "- PAY_AMT6: Amount of previous payment in April, 2005 (NT dollar)\n",
    "- default.payment.next.month: Default payment\n",
    "    - 1=yes\n",
    "    - 0=no"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "pd.set_option(\"display.max_rows\", 999)\n",
    "pd.set_option(\"display.max_columns\", 999)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"../data/default of credit card clients.xls\")\n",
    "new_header = df.iloc[0]\n",
    "df = df[1:] \n",
    "df.columns = new_header\n",
    "df = df.rename(columns={\"default payment next month\": \"default\"}) \n"
   ]
  },
  {
   "source": [
    "# Create Dataset Splits"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop([\"default\"], axis=1)\n",
    "y = df[\"default\"]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.8, random_state=42)\n",
    "X_tr, X_tt, y_tr, y_tt = train_test_split(X_train, y_train, train_size=0.875, random_state=42)\n",
    "train = pd.concat([X_tr, y_tr], axis=1)\n",
    "val = pd.concat([X_val, y_val], axis=1)\n",
    "tr = train.drop([\"ID\"], axis=1)\n",
    "val = val.drop([\"ID\"], axis=1)\n"
   ]
  },
  {
   "source": [
    "# Data Cleaning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://openexchangerates.org/api/latest.json?app_id=c51b1508fb4145259b1c2fade72a2c04'\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "rate = data['rates']['TWD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [tr, val]\n",
    "for d in data:\n",
    "    print(d.isna().sum())\n",
    "    d.rename(columns={\"PAY_0\": \"behind1\", \"PAY_2\": \"behind2\", \"PAY_3\": \"behind3\", \"PAY_4\": \"behind4\", \"PAY_5\": \"behind5\", \"PAY_6\": \"behind6\", \"BILL_AMT1\": \"billed1\", \"BILL_AMT2\": \"billed2\", \"BILL_AMT3\": \"billed3\", \"BILL_AMT4\": \"billed4\", \"BILL_AMT5\": \"billed5\", \"BILL_AMT6\": \"billed6\", \"PAY_AMT1\": \"paid1\", \"PAY_AMT2\": \"paid2\", \"PAY_AMT3\": \"paid3\", \"PAY_AMT4\": \"paid4\", \"PAY_AMT5\": \"paid5\", \"PAY_AMT6\": \"paid6\", \"SEX\": \"gender\", \"EDUCATION\": \"education\", \"MARRIAGE\": \"marriage\", \"AGE\": \"age\", \"LIMIT_BAL\": \"limit\"}, inplace=True)\n",
    "    d[['limit']] = d[['limit']]/rate\n",
    "    d[['billed1', 'billed2', 'billed3', 'billed4', 'billed5', 'billed6']] = d[['billed1', 'billed2', 'billed3', 'billed4', 'billed5', 'billed6']].divide(rate, axis=1)\n",
    "    d[['paid1', 'paid2', 'paid3', 'paid4', 'paid5', 'paid6']] = d[['paid1', 'paid2', 'paid3', 'paid4', 'paid5', 'paid6']].divide(rate, axis=1)\n",
    "    d['limit'] = d['limit'].apply(lambda x: round(x, 2))\n",
    "    d[['billed1', 'billed2', 'billed3', 'billed4', 'billed5', 'billed6']] = d[['billed1', 'billed2', 'billed3', 'billed4', 'billed5', 'billed6']].apply(lambda x: round(x, 2))\n",
    "    d[['paid1', 'paid2', 'paid3', 'paid4', 'paid5', 'paid6']] = d[['paid1', 'paid2', 'paid3', 'paid4', 'paid5', 'paid6']].apply(lambda x: round(x, 2))\n",
    "    d.replace({'marriage': {0:3}}, inplace=True)\n",
    "    d.replace({'education': {5:4, 0:4, 6:4}}, inplace=True)\n",
    "\n",
    "cols = ['behind1', 'behind2', 'behind3', 'behind4', 'behind5', 'behind6']\n",
    "for col in cols:\n",
    "    tr.replace({data: {-2: 0, -1: 0}}, inplace=True)\n",
    "    val.replace({data: {-2: 0, -1: 0}}, inplace=True)"
   ]
  },
  {
   "source": [
    "Observations:\n",
    "\n",
    "- No missing data\n",
    "- Reassign -2 and -1 to 0 for 'behind' features\n",
    "- "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Explaratory Data Anlaysis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# organize features into categorical and continuous\n",
    "categorical = tr[['gender', 'marriage', 'education', 'behind1', 'behind2', 'behind3', 'behind4', 'behind5', 'behind6']]\n",
    "continuous = tr[['limit', 'age', 'billed1', 'billed2', 'billed3', 'billed4', 'billed5', 'billed6', 'paid1', 'paid2', 'paid3', 'paid4', 'paid5', 'paid6']]\n",
    "cat_col = categorical.columns\n",
    "cont_col = continuous.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display distributions of all the continuous variables\n",
    "\n",
    "# con_1 = pd.melt(tr, value_vars = cont_col)\n",
    "# sns.set_theme(style=\"darkgrid\", font='serif', context='talk')\n",
    "# g = sns.FacetGrid(con_1, col='variable', col_wrap=3, sharex=False, sharey=False, height=4)\n",
    "# g = g.map(sns.distplot, 'value', color='r')\n",
    "# g.set_xticklabels(rotation=45)\n",
    "# g.fig.subplots_adjust(top=0.9)\n",
    "# g.fig.suptitle(\"Distributions of Continuous Features\")\n",
    "# g.fig.tight_layout()\n",
    "# plt.savefig(\"../images/distplot.png\")"
   ]
  },
  {
   "source": [
    "<img src=\"images/distplot.png\">"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "** Observations: **\n",
    "\n",
    "- "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use bar graphs of the distribution of data for categorical variables\n",
    "\n",
    "# cat_1 = pd.melt(tr, value_vars=cat_col)\n",
    "# sns.set_theme(style=\"darkgrid\", font='serif', context='talk')\n",
    "# g = sns.FacetGrid(cat_1, col='variable', col_wrap=3, sharex=False, sharey=False, height=4)\n",
    "# g = g.map(sns.countplot, 'value', color='dodgerblue')\n",
    "# g.set_xticklabels()\n",
    "# g.fig.subplots_adjust(top=0.9)\n",
    "# g.fig.suptitle(\"Distributions of Categorical Features\")\n",
    "# g.fig.tight_layout()\n",
    "# plt.savefig(\"../images/countplot.png\")"
   ]
  },
  {
   "source": [
    "<img src=\"images/countplot.png\">"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yes = tr.default.sum()\n",
    "no = len(tr)-yes\n",
    "perc_y = round(yes/len(tr)*100, 1)\n",
    "perc_n = round(no/len(tr)*100, 1)\n",
    "\n",
    "# plt.figure(figsize=(8,6))\n",
    "# sns.set_theme(style=\"darkgrid\", font='serif', context='talk')\n",
    "# sns.countplot('default', data=tr)\n",
    "# plt.title('Credit Card Baseline Default', size=16)\n",
    "# plt.box(False);\n",
    "# plt.savefig(\"../images/baseline.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of Total Non-Defaulters: \", yes)\n",
    "print(\"Number of Defaulters: \", no)\n",
    "print(\"Percentage of Non-Defaulters: \", perc_y)\n",
    "print(\"Percentage of Defaulters: \", perc_n)\n",
    "\n",
    "pd.DataFrame\n",
    "default = pd.DataFrame(data = {\"Training Dataset\": [yes, no, perc_y, perc_n]}, \n",
    "                       index = [\"Number of Total Non-Defaulters: \", \"Number of Defaulters: \", \"Percentage of Non-Defaulters: \", \"Percentage of Defaulters: \"])\n",
    "default"
   ]
  },
  {
   "source": [
    "The definition of imbalanced data is straightforward. A dataset is imbalanced if at least one of the classes constitutes only a very small minority. Imbalanced data prevail in banking, insurance, engineering, and many other fields. It is common in fraud detection that the imbalance is on the order of 100 to 1.\n",
    "\n",
    "The Receiver operating characteristic (ROC) curve is the typical tool for assessing the performance of machine learning algorithms, but it actually does not measure well for imbalanced data.\n",
    "Let’s formulate this decision problem with the labels as either positive or negative. The performance of a model can be represented in a confusion matrix with four categories. True positives (TP) are positive examples that are correctly labeled as positives, and False positives (FP) are negative examples that are labeled incorrectly as positive. Likewise, True negatives (TN) are negatives labeled correctly as negative, and false negatives (FN) refer to positive examples labeled incorrectly as negative.\n",
    "\n",
    "The above table shows TPR is TP / P, that depends only on positive cases. Because the Receiver Operating Characteristic (ROC) curves plot FPR vs. TPR (as shown below), the ROC curves do not measure the effects of negatives. The area under the ROC curve (AUC) assesses overall classification performance. AUC does not place more emphasis on one class over the other, so it does not reflect the minority class well. Remember that the red dashed line Figure A is the result when there is no model and the data are randomly drawn. The blue curve is the ROC curve. If the ROC curve is on top of the red dashed line, the AUC is 0.5 (half of the square area) and it means the model result is no different from a completely random draw. On the other hand, if the ROC curve is very close to the northwest corner, the AUC will be close to 1.0. So the AUC is a value between 1.0 (excellent fit) to 0.5 (random draw).\n",
    "\n",
    "Davis and Goadrich in this paper propose that Precision-Recall (PR) curves will be more informative than ROC when dealing with highly skewed datasets. The PR curves plot precision vs. recall (FPR). Because Precision is directly influenced by class imbalance so the Precision-recall curves are better to highlight differences between models for highly imbalanced data sets. When you compare different models with imbalanced settings, the area under the Precision-Recall curve will be more sensitive than the area under the ROC curve.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Using undersampling techniques\n",
    "(1) Random under-sampling for the majority class\n",
    "A simple under-sampling technique is to under-sample the majority class randomly and uniformly. This can potentially lead to loss of information. But if the examples of the majority class are near to others, this method might yield good results.\n",
    "(2) NearMiss\n",
    "In order to attack the issue of potential information loss, “near neighbor” method and its variations have been proposed. The basic algorithms of the near neighbor family are this: first, the method calculates the distances between all instances of the majority class and the instances of the minority class. Then k instances of the majority class that have the smallest distances to those in the minority class are selected. If there are n instances in the minority class, the “nearest” method will result in k*n instances of the majority class.\n",
    "“NearMiss-1” selects samples of the majority class that their average distances to three closest instances of the minority class are the smallest. “NearMiss-2” uses three farthest samples of the minority class. “NearMiss-3” selects a given number of the closest samples of the majority class for each sample of the minority class.\n",
    "(3) Condensed Nearest Neighbor Rule (CNN)\n",
    "To avoid losing potentially useful data, some heuristic undersampling methods have been proposed to remove redundant instances that should not affect the classification accuracy of the training set. Hart (1968) introduced the Condensed Nearest Neighbor Rule (CNN). Hart starts with two blank datasets A and B. Initially the first sample is placed in dataset A, while the rest samples are placed in dataset B. Then one instance from dataset B is scanned by using dataset A as the training set. If a point in B is misclassified, it is transferred from B to A. This process repeats until no points are transferred from B to A.\n",
    "(4) TomekLinks\n",
    "In the same manner, Tomek (1976) proposed an effective method that considers samples near the borderline. Given two instances a and b belonging to different classes and are separated by a distance d(a,b), the pair (a, b) is called a Tomek link if there is no instance c such that d(a,c) < d(a,b) or d(b,c) < d(a,b). Instances participating in Tomek links are either borderline or noise so both are removed.\n",
    "(5) Edited Nearest Neighbor Rule (ENN)\n",
    "Wilson (1972) introduced the Edited Nearest Neighbor Rule (ENN) to remove any instance whose class label is different from the class of at least two of its three nearest neighbors. The idea behind this technique is to remove the instances from the majority class that is near or around the borderline of different classes based on the concept of nearest neighbor (NN) in order to increase classification accuracy of minority instances rather than majority instances.\n",
    "(6) NeighbourhoodCleaningRule\n",
    "Neighborhood Cleaning Rule (NCL) deals with the majority and minority samples separately when sampling the data sets. NCL uses ENN to remove majority examples. for each instance in the training set, it finds three nearest neighbors. If the instance belongs to the majority class and the classification given by its three nearest neighbors is the opposite of the class of the chosen instance, then the chosen instance is removed. If the chosen instance belongs to the minority class and is misclassified by its three nearest neighbors, then the nearest neighbors that belong to the majority class are removed.\n",
    "(7) ClusterCentroids\n",
    "This method undersamples the majority class by replacing a cluster of majority samples This method finds the clusters of majority class with K-mean algorithms. Then it keeps the cluster centroids of the N clusters as the new majority samples."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}