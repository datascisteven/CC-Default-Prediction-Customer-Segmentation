{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# <a id='0'>Contents</a>\n",
    "\n",
    "- <a href='#2'>Importing Packages</a>  \n",
    "- <a href='#3'>Uploading Data</a>\n",
    "- <a href='#4'>Creating Train, Validation, and Testing Sets</a>  \n",
    "- <a href='#5'>Data Cleaning</a>  \n",
    "- <a href='#5'>Exploratory Data Analysis</a>\n",
    "- <a href='#6'>Feature Engineering</a>  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Introduction\n",
    "\n",
    "This dataset contains information on default payments, demographic factors, credit data, history of payment, and bill statements of credit card customers in Taiwan from April 2005 to September 2005.\n",
    "\n",
    "\n",
    "There are 25 variables:\n",
    "\n",
    "- ID: ID of each client\n",
    "- LIMIT_BAL: Amount of given credit in NT dollars (includes individual and family/supplementary credit\n",
    "- SEX: Gender\n",
    "    - 1=male\n",
    "    - 2=female\n",
    "- EDUCATION:\n",
    "    - 1=graduate school\n",
    "    - 2=university\n",
    "    - 3=high school\n",
    "    - 4=others\n",
    "    - 5=unknown\n",
    "    - 6=unknown\n",
    "- MARRIAGE: Marital status\n",
    "    - 1=married\n",
    "    - 2=single\n",
    "    - 3=others)\n",
    "- AGE: Age in years\n",
    "- PAY_0: Repayment status in September, 2005\n",
    "    - -1=pay duly\n",
    "    - 1=payment delay for one month\n",
    "    - 2=payment delay for two months\n",
    "    ....\n",
    "    - 8=payment delay for eight months\n",
    "    - 9=payment delay for nine months and above\n",
    "- PAY_2: Repayment status in August, 2005 (scale same as above)\n",
    "- PAY_3: Repayment status in July, 2005 (scale same as above)\n",
    "- PAY_4: Repayment status in June, 2005 (scale same as above)\n",
    "- PAY_5: Repayment status in May, 2005 (scale same as above)\n",
    "- PAY_6: Repayment status in April, 2005 (scale same as above)\n",
    "- BILL_AMT1: Amount of bill statement in September,2005 (NT dollar)\n",
    "- BILL_AMT2: Amount of bill statement in August, 2005 (NT dollar)\n",
    "- BILL_AMT3: Amount of bill statement in July, 2005 (NT dollar)\n",
    "- BILL_AMT4: Amount of bill statement in June, 2005 (NT dollar)\n",
    "- BILL_AMT5: Amount of bill statement in May, 2005 (NT dollar)\n",
    "- BILL_AMT6: Amount of bill statement in April, 2005 (NT dollar)\n",
    "- PAY_AMT1: Amount of previous payment in September, 2005 (NT dollar)\n",
    "- PAY_AMT2: Amount of previous payment in August, 2005 (NT dollar)\n",
    "- PAY_AMT3: Amount of previous payment in July, 2005 (NT dollar)\n",
    "- PAY_AMT4: Amount of previous payment in June, 2005 (NT dollar)\n",
    "- PAY_AMT5: Amount of previous payment in May, 2005 (NT dollar)\n",
    "- PAY_AMT6: Amount of previous payment in April, 2005 (NT dollar)\n",
    "- default.payment.next.month: Default payment\n",
    "    - 1=yes\n",
    "    - 0=no"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Importing Packages"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Packages\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option(\"display.max_rows\", 999)\n",
    "pd.set_option(\"display.max_columns\", 999)\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "import pickle\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier, BaggingClassifier\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, cross_val_predict, GridSearchCV\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score, classification_report,balanced_accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"data/default of credit card clients.xls\")\n",
    "new_header = df.iloc[0]\n",
    "df = df[1:] \n",
    "df.columns = new_header\n",
    "df = df.rename(columns={\"default payment next month\": \"default\"}) "
   ]
  },
  {
   "source": [
    "# Create Dataset Splits"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop([\"default\"], axis=1)\n",
    "y = df[\"default\"]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.8, random_state=42)\n",
    "X_tr, X_tt, y_tr, y_tt = train_test_split(X_train, y_train, train_size=0.875, random_state=42)\n",
    "train = pd.concat([X_tr, y_tr], axis=1)\n",
    "val = pd.concat([X_val, y_val], axis=1)\n",
    "tr = train.drop([\"ID\"], axis=1)\n",
    "val = val.drop([\"ID\"], axis=1)"
   ]
  },
  {
   "source": [
    "# Data Cleaning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://openexchangerates.org/api/latest.json?app_id=c51b1508fb4145259b1c2fade72a2c04'\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "rate = data['rates']['TWD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [tr, val]\n",
    "for d in data:\n",
    "    d.rename(columns={\"PAY_0\": \"behind1\", \"PAY_2\": \"behind2\", \"PAY_3\": \"behind3\", \"PAY_4\": \"behind4\", \"PAY_5\": \"behind5\", \"PAY_6\": \"behind6\", \"BILL_AMT1\": \"billed1\", \"BILL_AMT2\": \"billed2\", \"BILL_AMT3\": \"billed3\", \"BILL_AMT4\": \"billed4\", \"BILL_AMT5\": \"billed5\", \"BILL_AMT6\": \"billed6\", \"PAY_AMT1\": \"paid1\", \"PAY_AMT2\": \"paid2\", \"PAY_AMT3\": \"paid3\", \"PAY_AMT4\": \"paid4\", \"PAY_AMT5\": \"paid5\", \"PAY_AMT6\": \"paid6\", \"SEX\": \"gender\", \"EDUCATION\": \"education\", \"MARRIAGE\": \"marriage\", \"AGE\": \"age\", \"LIMIT_BAL\": \"limit\"}, inplace=True)\n",
    "    d[['limit']] = d[['limit']]/rate\n",
    "    d[['billed1', 'billed2', 'billed3', 'billed4', 'billed5', 'billed6']] = d[['billed1', 'billed2', 'billed3', 'billed4', 'billed5', 'billed6']].divide(rate, axis=1).astype(int)\n",
    "    d[['paid1', 'paid2', 'paid3', 'paid4', 'paid5', 'paid6']] = d[['paid1', 'paid2', 'paid3', 'paid4', 'paid5', 'paid6']].divide(rate, axis=1).astype(int)\n",
    "    d['limit'] = d['limit'].apply(lambda x: round(x, 2))\n",
    "    d.replace({'marriage': {0:3}}, inplace=True)\n",
    "    d.replace({'education': {5:4, 0:4, 6:4}}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         limit gender  education  marriage age behind1 behind2 behind3  \\\n",
       "6191   1791.50      2          2         1  44       0       0       0   \n",
       "16054  5732.81      2          3         1  46      -1      -1      -1   \n",
       "19706  3583.01      2          2         1  47      -1      -1      -1   \n",
       "23128  6091.12      2          2         1  29       0       0       0   \n",
       "28516  5374.51      2          1         2  33      -2      -2      -2   \n",
       "\n",
       "      behind4 behind5 behind6  billed1  billed2  billed3  billed4  billed5  \\\n",
       "6191        0       0       0     1633     1501     1279      801      847   \n",
       "16054       0      -1      -1      892       83      173      147      143   \n",
       "19706      -1      -1      -2      238      238        0      224      -14   \n",
       "23128       0       0       0     2833     2242     2268     2289     1558   \n",
       "28516      -2      -2      -2      874      961     1171     1198      996   \n",
       "\n",
       "       billed6  paid1  paid2  paid3  paid4  paid5  paid6 default  \n",
       "6191       982    108    179    107    107    179     33       0  \n",
       "16054       30     83    173     35    143     30    942       0  \n",
       "19706      -14    238      0    224      0      0      0       1  \n",
       "23128     1576     80     89     92     60     68     75       0  \n",
       "28516       81    967   1172   1199    996     81   6071       0  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>limit</th>\n      <th>gender</th>\n      <th>education</th>\n      <th>marriage</th>\n      <th>age</th>\n      <th>behind1</th>\n      <th>behind2</th>\n      <th>behind3</th>\n      <th>behind4</th>\n      <th>behind5</th>\n      <th>behind6</th>\n      <th>billed1</th>\n      <th>billed2</th>\n      <th>billed3</th>\n      <th>billed4</th>\n      <th>billed5</th>\n      <th>billed6</th>\n      <th>paid1</th>\n      <th>paid2</th>\n      <th>paid3</th>\n      <th>paid4</th>\n      <th>paid5</th>\n      <th>paid6</th>\n      <th>default</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>6191</th>\n      <td>1791.50</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>44</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1633</td>\n      <td>1501</td>\n      <td>1279</td>\n      <td>801</td>\n      <td>847</td>\n      <td>982</td>\n      <td>108</td>\n      <td>179</td>\n      <td>107</td>\n      <td>107</td>\n      <td>179</td>\n      <td>33</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>16054</th>\n      <td>5732.81</td>\n      <td>2</td>\n      <td>3</td>\n      <td>1</td>\n      <td>46</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>892</td>\n      <td>83</td>\n      <td>173</td>\n      <td>147</td>\n      <td>143</td>\n      <td>30</td>\n      <td>83</td>\n      <td>173</td>\n      <td>35</td>\n      <td>143</td>\n      <td>30</td>\n      <td>942</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>19706</th>\n      <td>3583.01</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>47</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-2</td>\n      <td>238</td>\n      <td>238</td>\n      <td>0</td>\n      <td>224</td>\n      <td>-14</td>\n      <td>-14</td>\n      <td>238</td>\n      <td>0</td>\n      <td>224</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>23128</th>\n      <td>6091.12</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>29</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2833</td>\n      <td>2242</td>\n      <td>2268</td>\n      <td>2289</td>\n      <td>1558</td>\n      <td>1576</td>\n      <td>80</td>\n      <td>89</td>\n      <td>92</td>\n      <td>60</td>\n      <td>68</td>\n      <td>75</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>28516</th>\n      <td>5374.51</td>\n      <td>2</td>\n      <td>1</td>\n      <td>2</td>\n      <td>33</td>\n      <td>-2</td>\n      <td>-2</td>\n      <td>-2</td>\n      <td>-2</td>\n      <td>-2</td>\n      <td>-2</td>\n      <td>874</td>\n      <td>961</td>\n      <td>1171</td>\n      <td>1198</td>\n      <td>996</td>\n      <td>81</td>\n      <td>967</td>\n      <td>1172</td>\n      <td>1199</td>\n      <td>996</td>\n      <td>81</td>\n      <td>6071</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "tr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "              limit     education      marriage       billed1       billed2  \\\n",
       "count  21000.000000  21000.000000  21000.000000  21000.000000  21000.000000   \n",
       "mean    5991.325979      1.842810      1.555333   1831.031952   1761.133667   \n",
       "std     4642.188925      0.746378      0.522538   2632.407699   2550.197573   \n",
       "min      358.300000      1.000000      1.000000  -5932.000000  -2500.000000   \n",
       "25%     1791.500000      1.000000      1.000000    127.000000    108.000000   \n",
       "50%     5016.210000      2.000000      2.000000    804.000000    767.000000   \n",
       "75%     8599.220000      2.000000      2.000000   2391.250000   2267.500000   \n",
       "max    35830.090000      4.000000      3.000000  34558.000000  35254.000000   \n",
       "\n",
       "            billed3       billed4       billed5       billed6         paid1  \\\n",
       "count  21000.000000  21000.000000  21000.000000  21000.000000  21000.000000   \n",
       "mean    1682.417905   1546.528190   1446.264762   1396.106143    204.954286   \n",
       "std     2493.955364   2303.286683   2185.746166   2138.160074    627.397874   \n",
       "min    -5634.000000  -6091.000000  -2914.000000  -7490.000000      0.000000   \n",
       "25%       98.000000     84.000000     63.000000     46.000000     35.000000   \n",
       "50%      719.000000    682.000000    648.000000    613.000000     75.000000   \n",
       "75%     2138.250000   1947.000000   1801.000000   1770.000000    179.000000   \n",
       "max    59624.000000  31945.000000  33220.000000  34456.000000  31299.000000   \n",
       "\n",
       "              paid2         paid3         paid4         paid5         paid6  \n",
       "count  21000.000000  21000.000000  21000.000000  21000.000000  21000.000000  \n",
       "mean     214.768381    188.736667    176.128000    172.372143    184.361381  \n",
       "std      898.821276    668.116436    602.046148    559.641402    632.686513  \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000  \n",
       "25%       29.000000     13.000000     10.000000      8.000000      4.000000  \n",
       "50%       71.000000     64.000000     53.000000     53.000000     53.000000  \n",
       "75%      179.000000    161.000000    143.000000    144.000000    143.000000  \n",
       "max    60347.000000  32105.000000  22250.000000  14976.000000  18887.000000  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>limit</th>\n      <th>education</th>\n      <th>marriage</th>\n      <th>billed1</th>\n      <th>billed2</th>\n      <th>billed3</th>\n      <th>billed4</th>\n      <th>billed5</th>\n      <th>billed6</th>\n      <th>paid1</th>\n      <th>paid2</th>\n      <th>paid3</th>\n      <th>paid4</th>\n      <th>paid5</th>\n      <th>paid6</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>21000.000000</td>\n      <td>21000.000000</td>\n      <td>21000.000000</td>\n      <td>21000.000000</td>\n      <td>21000.000000</td>\n      <td>21000.000000</td>\n      <td>21000.000000</td>\n      <td>21000.000000</td>\n      <td>21000.000000</td>\n      <td>21000.000000</td>\n      <td>21000.000000</td>\n      <td>21000.000000</td>\n      <td>21000.000000</td>\n      <td>21000.000000</td>\n      <td>21000.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>5991.325979</td>\n      <td>1.842810</td>\n      <td>1.555333</td>\n      <td>1831.031952</td>\n      <td>1761.133667</td>\n      <td>1682.417905</td>\n      <td>1546.528190</td>\n      <td>1446.264762</td>\n      <td>1396.106143</td>\n      <td>204.954286</td>\n      <td>214.768381</td>\n      <td>188.736667</td>\n      <td>176.128000</td>\n      <td>172.372143</td>\n      <td>184.361381</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>4642.188925</td>\n      <td>0.746378</td>\n      <td>0.522538</td>\n      <td>2632.407699</td>\n      <td>2550.197573</td>\n      <td>2493.955364</td>\n      <td>2303.286683</td>\n      <td>2185.746166</td>\n      <td>2138.160074</td>\n      <td>627.397874</td>\n      <td>898.821276</td>\n      <td>668.116436</td>\n      <td>602.046148</td>\n      <td>559.641402</td>\n      <td>632.686513</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>358.300000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>-5932.000000</td>\n      <td>-2500.000000</td>\n      <td>-5634.000000</td>\n      <td>-6091.000000</td>\n      <td>-2914.000000</td>\n      <td>-7490.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>1791.500000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>127.000000</td>\n      <td>108.000000</td>\n      <td>98.000000</td>\n      <td>84.000000</td>\n      <td>63.000000</td>\n      <td>46.000000</td>\n      <td>35.000000</td>\n      <td>29.000000</td>\n      <td>13.000000</td>\n      <td>10.000000</td>\n      <td>8.000000</td>\n      <td>4.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>5016.210000</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>804.000000</td>\n      <td>767.000000</td>\n      <td>719.000000</td>\n      <td>682.000000</td>\n      <td>648.000000</td>\n      <td>613.000000</td>\n      <td>75.000000</td>\n      <td>71.000000</td>\n      <td>64.000000</td>\n      <td>53.000000</td>\n      <td>53.000000</td>\n      <td>53.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>8599.220000</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>2391.250000</td>\n      <td>2267.500000</td>\n      <td>2138.250000</td>\n      <td>1947.000000</td>\n      <td>1801.000000</td>\n      <td>1770.000000</td>\n      <td>179.000000</td>\n      <td>179.000000</td>\n      <td>161.000000</td>\n      <td>143.000000</td>\n      <td>144.000000</td>\n      <td>143.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>35830.090000</td>\n      <td>4.000000</td>\n      <td>3.000000</td>\n      <td>34558.000000</td>\n      <td>35254.000000</td>\n      <td>59624.000000</td>\n      <td>31945.000000</td>\n      <td>33220.000000</td>\n      <td>34456.000000</td>\n      <td>31299.000000</td>\n      <td>60347.000000</td>\n      <td>32105.000000</td>\n      <td>22250.000000</td>\n      <td>14976.000000</td>\n      <td>18887.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "tr.describe()"
   ]
  },
  {
   "source": [
    "** Observations: **\n",
    "\n",
    "- No missing data\n",
    "- There were anomalous values for education and marriage, and the anomalous values were reassigned under other.\n",
    "- Did not reassign -2 and -1 to 0 for 'behind' features despite being anomalous because they were so many -2 and -1.  There must be so significance to those values."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Exploratory Data Anlaysis"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# organize features into categorical and continuous\n",
    "categorical = tr[['gender', 'marriage', 'education', 'behind1', 'behind2', 'behind3', 'behind4', 'behind5', 'behind6']]\n",
    "continuous = tr[['limit', 'age', 'billed1', 'billed2', 'billed3', 'billed4', 'billed5', 'billed6', 'paid1', 'paid2', 'paid3', 'paid4', 'paid5', 'paid6']]\n",
    "cat_col = categorical.columns\n",
    "cont_col = continuous.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display distributions of all the continuous variables\n",
    "\n",
    "# con_1 = pd.melt(tr, value_vars = cont_col)\n",
    "# sns.set_theme(style=\"darkgrid\", font='serif', context='talk')\n",
    "# g = sns.FacetGrid(con_1, col='variable', col_wrap=3, sharex=False, sharey=False, height=4)\n",
    "# g = g.map(sns.distplot, 'value', color='r')\n",
    "# g.set_xticklabels(rotation=45)\n",
    "# g.fig.subplots_adjust(top=0.9)\n",
    "# g.fig.suptitle(\"Distributions of Continuous Features\")\n",
    "# g.fig.tight_layout()\n",
    "# plt.savefig(\"../images/distplot.png\")"
   ]
  },
  {
   "source": [
    "<img src=\"images/distplot.png\">"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "** Observations: **\n",
    "\n",
    "- "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use bar graphs of the distribution of data for categorical variables\n",
    "\n",
    "# cat_1 = pd.melt(tr, value_vars=cat_col)\n",
    "# sns.set_theme(style=\"darkgrid\", font='serif', context='talk')\n",
    "# g = sns.FacetGrid(cat_1, col='variable', col_wrap=3, sharex=False, sharey=False, height=4)\n",
    "# g = g.map(sns.countplot, 'value', color='dodgerblue')\n",
    "# g.set_xticklabels()\n",
    "# g.fig.subplots_adjust(top=0.9)\n",
    "# g.fig.suptitle(\"Distributions of Categorical Features\")\n",
    "# g.fig.tight_layout()\n",
    "# plt.savefig(\"../images/countplot.png\")"
   ]
  },
  {
   "source": [
    "<img src=\"images/countplot.png\">"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "yes = tr.default.sum()\n",
    "no = len(tr)-yes\n",
    "perc_y = round(yes/len(tr)*100, 1)\n",
    "perc_n = round(no/len(tr)*100, 1)\n",
    "\n",
    "# plt.figure(figsize=(8,6))\n",
    "# sns.set_theme(style=\"darkgrid\", font='serif', context='talk')\n",
    "# sns.countplot('default', data=tr)\n",
    "# plt.title('Credit Card Baseline Default', size=16)\n",
    "# plt.box(False);\n",
    "# plt.savefig(\"../images/baseline.png\")"
   ]
  },
  {
   "source": [
    "<img src=\"images/baseline.png\">"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of Total Non-Defaulters:  4656\nNumber of Defaulters:  16344\nPercentage of Non-Defaulters:  22.2\nPercentage of Defaulters:  77.8\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                  Training Dataset\n",
       "Number of Total Non-Defaulters:             4656.0\n",
       "Number of Defaulters:                      16344.0\n",
       "Percentage of Non-Defaulters:                 22.2\n",
       "Percentage of Defaulters:                     77.8"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Training Dataset</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Number of Total Non-Defaulters:</th>\n      <td>4656.0</td>\n    </tr>\n    <tr>\n      <th>Number of Defaulters:</th>\n      <td>16344.0</td>\n    </tr>\n    <tr>\n      <th>Percentage of Non-Defaulters:</th>\n      <td>22.2</td>\n    </tr>\n    <tr>\n      <th>Percentage of Defaulters:</th>\n      <td>77.8</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "print(\"Number of Total Non-Defaulters: \", yes)\n",
    "print(\"Number of Defaulters: \", no)\n",
    "print(\"Percentage of Non-Defaulters: \", perc_y)\n",
    "print(\"Percentage of Defaulters: \", perc_n)\n",
    "\n",
    "pd.DataFrame\n",
    "default = pd.DataFrame(data = {\"Training Dataset\": [yes, no, perc_y, perc_n]}, \n",
    "                       index = [\"Number of Total Non-Defaulters: \", \"Number of Defaulters: \", \"Percentage of Non-Defaulters: \", \"Percentage of Defaulters: \"])\n",
    "default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset = tr[['gender', 'education', 'marriage', 'behind1', 'behind2', 'behind3', 'behind4', 'behind5', 'behind6', 'default']]\n",
    "# f, axes = plt.subplots(3, 3, figsize=(15, 12), facecolor='white')\n",
    "# sns.set_theme(style=\"darkgrid\", font='serif', context='paper')\n",
    "# f.suptitle('Frequency of Categorical Variables', size=16)\n",
    "# ax1 = sns.countplot(x=\"gender\", hue=\"default\", data=subset, ax=axes[0,0])\n",
    "# ax2 = sns.countplot(x=\"education\", hue=\"default\", data=subset, ax=axes[0,1])\n",
    "# ax3 = sns.countplot(x=\"marriage\", hue=\"default\", data=subset, ax=axes[0,2])\n",
    "# ax4 = sns.countplot(x=\"behind1\", hue=\"default\", data=subset, ax=axes[1,0])\n",
    "# ax5 = sns.countplot(x=\"behind2\", hue=\"default\", data=subset, ax=axes[1,1])\n",
    "# ax6 = sns.countplot(x=\"behind3\", hue=\"default\", data=subset, ax=axes[1,2])\n",
    "# ax7 = sns.countplot(x=\"behind4\", hue=\"default\", data=subset, ax=axes[2,0])\n",
    "# ax8 = sns.countplot(x=\"behind5\", hue=\"default\", data=subset, ax=axes[2,1])\n",
    "# ax9 = sns.countplot(x=\"behind6\", hue=\"default\", data=subset, ax=axes[2,2])\n",
    "# plt.savefig(\"../images/default_freq_by_cat.png\")"
   ]
  },
  {
   "source": [
    "<img src=\"images/default_freq_by_cat.png\">"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "** Observations: **\n",
    "\n",
    "- `gender`, `education`, and `marriage` doesn't seem to change with each group in terms of proportions.  Behind seems to have some correlation with default.  That would make sense since being behind in payments would make it more likely that you would default next month. \n",
    "\n",
    "- There isn’t a very clear distinction between the distribution of `default` on any of the demographic data. However, you do see quite some distribution differences of target classes for monthly repayment status (`behind1`-`behind6`).\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "default       0     1\n",
       "education            \n",
       "1          6013  1424\n",
       "2          7408  2341\n",
       "3          2626   866\n",
       "4           297    25"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>default</th>\n      <th>0</th>\n      <th>1</th>\n    </tr>\n    <tr>\n      <th>education</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>6013</td>\n      <td>1424</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7408</td>\n      <td>2341</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2626</td>\n      <td>866</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>297</td>\n      <td>25</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "education = tr.groupby(['education', 'default']).size().unstack(1)\n",
    "education\n",
    "# education.plot(kind=\"bar\", stacked=True)\n",
    "# plt.title(\"Distribution Count of Educational Level and Default Status\", size=14)\n",
    "# plt.savefig(\"../data/stacked_bar2.png\")"
   ]
  },
  {
   "source": [
    "<img src=\"images/stacked_bar2.png\">"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "** No clear relationship with education. The proportion doesn't seem to change with each group. **"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "default      0     1\n",
       "marriage            \n",
       "1         7354  2258\n",
       "2         8778  2336\n",
       "3          212    62"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>default</th>\n      <th>0</th>\n      <th>1</th>\n    </tr>\n    <tr>\n      <th>marriage</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>7354</td>\n      <td>2258</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8778</td>\n      <td>2336</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>212</td>\n      <td>62</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "marriage = tr.groupby(['marriage', 'default']).size().unstack(1)\n",
    "marriage\n",
    "# marriage.plot(kind=\"bar\", stacked=True)\n",
    "# plt.title(\"Distribution of Default Status for Marital Status\", size=14)\n",
    "# plt.savefig(\"../images/stacked_bar3.png\")"
   ]
  },
  {
   "source": [
    "<img src=\"images/stacked_bar3.png\">"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Feature Engineering\n",
    "\n",
    "- `age_bin`: 1 = young adult, 2 = middle age, 3 = senior\n",
    "- `gen-mar`: interaction between gender and marriage status\n",
    "- `gen-age`: interaction between age and gender\n",
    "- 'avail...`: fraction of estimated available balance based on what is billed per month"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [tr, val]\n",
    "\n",
    "# create features for demographic variables\n",
    "for d in data:\n",
    "    d['age_bin'] = 0\n",
    "    d.loc[((d['age'] > 20) & (d['age'] < 30)) , 'age_bin'] = 1\n",
    "    d.loc[((d['age'] >= 30) & (d['age'] < 60)) , 'age_bin'] = 2\n",
    "    d.loc[((d['age'] >= 60) & (d['age'] < 81)) , 'age_bin'] = 3\n",
    "    # create categories for single, married, divorced males and females\n",
    "    d['gen-mar'] = d['gender'] + d['marriage']\n",
    "    # create categories for young, middle age and senior males and females\n",
    "    d['gen-age'] = d['gender'] + d['age_bin']\n",
    "\n",
    "# feature for credit use percentage: fraction of estimated available balance based on what is billed per month\n",
    "# (credit limit - monthly billed amount) / credit limit\n",
    "for d in data:\n",
    "    d['avail6'] = (d.limit - d.billed6) / d.limit\n",
    "    d['avail5'] = (d.limit - d.billed5) / d.limit\n",
    "    d['avail4'] = (d.limit - d.billed4) / d.limit\n",
    "    d['avail3'] = (d.limit - d.billed3) / d.limit\n",
    "    d['avail2'] = (d.limit - d.billed2) / d.limit\n",
    "    d['avail1'] = (d.limit - d.billed1) / d.limit\n",
    "    d['avg_av'] = (d.avail1 + d.avail2 + d.avail3 + d.avail4 + d.avail5 + d.avail6) / 6\n",
    "\n",
    "# create a feature that indicates whether a client has had a delayed payment or not\n",
    "def delayed_payment(d):\n",
    "    if (d.behind1 > 0) or (d.behind2 > 0) or (d.behind3 > 0) or (d.behind4 > 0) or (d.behind5 > 0) or (d.behind6 > 0):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "for d in data:\n",
    "    d['delayed'] = d.apply(delayed_payment, axis=1)\n",
    "\n",
    "# create feature for the total number of months with delayed payment status for a particular client\n",
    "def total_months_with_delayed_payments(d):\n",
    "    count = 0\n",
    "    if (d.behind1 > 0):\n",
    "        count += 1\n",
    "    if (d.behind2 > 0):\n",
    "        count += 1\n",
    "    if (d.behind3 > 0):\n",
    "        count += 1\n",
    "    if (d.behind4 > 0):\n",
    "        count += 1\n",
    "    if (d.behind5 > 0):\n",
    "        count += 1\n",
    "    if (d.behind6 > 0):\n",
    "        count += 1\n",
    "    return count\n",
    "for d in data:\n",
    "    d['latemths'] = d.apply(total_months_with_delayed_payments, axis=1)\n",
    "\n",
    "# the ratio of amount paid and amount billed\n",
    "for d in data:\n",
    "    d['pperb1'] = d.paid1 / d.billed2\n",
    "    d['pperb2'] = d.paid2 / d.billed3\n",
    "    d['pperb3'] = d.paid3 / d.billed4\n",
    "    d['pperb4'] = d.paid4 / d.billed5\n",
    "    d['pperb5'] = d.paid5 / d.billed6\n",
    "\n",
    "# remove any infinity and NaN values\n",
    "datasets = ['pperb1', 'pperb2', 'pperb3', 'pperb4', 'pperb5']\n",
    "for data in datasets:\n",
    "    tr.replace({data: {np.inf: 0, np.nan: 0}}, inplace=True)\n",
    "    val.replace({data: {np.inf: 0, np.nan: 0}}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.style.use(\"fivethirtyeight\")\n",
    "# sns.set_theme(style=\"darkgrid\", font='serif', context='paper')\n",
    "# plt.figure(figsize = (20,16))\n",
    "# plt.title('Pearson Correlation of Features', y = 1.05, size = 20)\n",
    "# g = sns.heatmap(tr.corr(), cmap='RdBu', square=True, linecolor='white', linewidths=0.2)\n",
    "# plt.savefig(\"../images/correlation_matrix_2.png\")\n"
   ]
  },
  {
   "source": [
    "<img src=\"images/correlation_matrix_2.png\">"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "** This includes my engineered features.  Default seems to be correlated with two of my engineered features, delayed and latemnths.  Delayed is whether you have had a delayed payment durig the 6 month history or not. latemnths is the total nunber of months you were given a status of behind in payments.  Seems to be correlated with behind1 and limit.**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open(\"../data/training_features.pickle\",\"rb\")\n",
    "train2 = pickle.load(pickle_in)\n",
    "pickle_in = open(\"../data/validate_features.pickle\",\"rb\")\n",
    "validate2 = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2 = train2.drop([\"default\"], axis=1)\n",
    "y_tr = train2[\"default\"]\n",
    "X_validate2 = validate2.drop([\"default\"], axis=1)\n",
    "y_val = validate2[\"default\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Grab indices of columns for creating dummy variables and create dataframe with dummy variables\n",
    "dum_feat = X_train2[['gender', 'education', 'marriage', 'age_bin', 'gen-mar', 'gen-age']]\n",
    "dum_index = dum_feat.columns\n",
    "tr_dum = pd.get_dummies(data=dum_feat, columns=dum_index, drop_first=True, prefix=['sex', 'edu', 'mar', 'agebin', 'sexmar', 'sexage'])\n",
    "cont_feat = X_train2.drop(['gender', 'education', 'marriage', 'age_bin', 'gen-mar', 'gen-age'], axis=1)\n",
    "X_train2_dum = cont_feat.join(tr_dum)\n",
    "X_train2_dum.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dum_feat2 = X_validate2[['gender', 'education', 'marriage', 'age_bin', 'gen-mar', 'gen-age']]\n",
    "dum_index2 = dum_feat2.columns\n",
    "val_dum = pd.get_dummies(data=dum_feat2, columns=dum_index2, drop_first=True, prefix=['sex', 'edu', 'mar', 'agebin', 'sexmar', 'sexage'])\n",
    "cont_feat2 = X_validate2.drop(['gender', 'education', 'marriage', 'age_bin', 'gen-mar', 'gen-age'], axis=1)\n",
    "X_validate2_dum = cont_feat2.join(val_dum)\n",
    "X_validate2_dum.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X_train2_dum)\n",
    "X_tr2_dum = scaler.transform(X_train2_dum)\n",
    "X_val2_dum = scaler.transform(X_validate2_dum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler2 = StandardScaler().fit(X_train2)\n",
    "X_tr2 = scaler2.transform(X_train2)\n",
    "X_val2 = scaler2.transform(X_validate2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc2 = RandomForestClassifier().fit(X_tr2, y_tr)\n",
    "y_pred_rfc_tr2 = rfc2.predict(X_tr2)\n",
    "y_pred_rfc_val2 = rfc2.predict(X_val2)\n",
    "get_metrics(X_tr2, y_tr, X_val2, y_val, y_pred_rfc_tr2, y_pred_rfc_val2, rfc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc2 = DecisionTreeClassifier().fit(X_tr2, y_tr)\n",
    "y_pred_dtc_tr2 = dtc2.predict(X_tr2)\n",
    "y_pred_dtc_val2 = dtc2.predict(X_val2)\n",
    "get_metrics(X_tr2, y_tr, X_val2, y_val, y_pred_dtc_tr2, y_pred_dtc_val2, dtc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc2 = AdaBoostClassifier().fit(X_tr2, y_tr)\n",
    "y_pred_abc_tr2 = abc2.predict(X_tr2)\n",
    "y_pred_abc_val2 = abc2.predict(X_val2)\n",
    "get_metrics(X_tr2, y_tr, X_val2, y_val, y_pred_abc_tr2, y_pred_abc_val2, abc2)"
   ]
  },
  {
   "source": [
    "# New Baseline Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open(\"../data/training_model.pickle\",\"rb\")\n",
    "train3 = pickle.load(pickle_in)\n",
    "pickle_in = open(\"../data/validate_model.pickle\",\"rb\")\n",
    "validate3 = pickle.load(pickle_in)\n",
    "\n",
    "X_train3 = train3.drop([\"default\"], axis=1)\n",
    "y_tr = train3[\"default\"]\n",
    "X_validate3 = validate3.drop([\"default\"], axis=1)\n",
    "y_val = validate3[\"default\"]\n",
    "\n",
    "scaler3 = StandardScaler().fit(X_train3)\n",
    "X_tr3 = scaler3.transform(X_train3)\n",
    "X_val3 = scaler3.transform(X_validate3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg3 = LogisticRegression(solver=\"liblinear\", random_state=42).fit(X_tr3, y_tr)\n",
    "y_pred_log_tr3 = logreg3.predict(X_tr3)\n",
    "y_pred_log_val3 = logreg3.predict(X_val3)\n",
    "get_metrics(X_tr3, y_tr, X_val3, y_val, y_pred_log_tr3, y_pred_log_val3, logreg3)\n",
    "\n",
    "rfc3 = RandomForestClassifier().fit(X_tr3, y_tr)\n",
    "y_pred_rfc_tr3 = rfc3.predict(X_tr3)\n",
    "y_pred_rfc_val3 = rfc3.predict(X_val3)\n",
    "get_metrics(X_tr3, y_tr, X_val3, y_val, y_pred_rfc_tr3, y_pred_rfc_val3, rfc3)\n",
    "\n",
    "dtc3 = DecisionTreeClassifier().fit(X_tr3, y_tr)\n",
    "y_pred_dtc_tr3 = dtc3.predict(X_tr3)\n",
    "y_pred_dtc_val3 = dtc3.predict(X_val3)\n",
    "get_metrics(X_tr3, y_tr, X_val3, y_val, y_pred_dtc_tr3, y_pred_dtc_val3, dtc3)\n",
    "\n",
    "abc3 = AdaBoostClassifier().fit(X_tr3, y_tr)\n",
    "y_pred_abc_tr3 = abc3.predict(X_tr3)\n",
    "y_pred_abc_val3 = abc3.predict(X_val3)\n",
    "get_metrics(X_tr3, y_tr, X_val3, y_val, y_pred_abc_tr3, y_pred_abc_val3, abc3)\n",
    "\n",
    "gbc3 = GradientBoostingClassifier().fit(X_tr3, y_tr)\n",
    "y_pred_gbc_tr3 = gbc3.predict(X_tr3)\n",
    "y_pred_gbc_val3 = gbc3.predict(X_val3)\n",
    "get_metrics(X_tr3, y_tr, X_val3, y_val, y_pred_gbc_tr3, y_pred_gbc_val3, gbc3)\n",
    "\n",
    "xgb3 = XGBClassifier().fit(X_tr3, y_tr)\n",
    "y_pred_xgb_tr3 = xgb3.predict(X_tr3)\n",
    "y_pred_xgb_val3 = xgb3.predict(X_val3)\n",
    "get_metrics(X_tr3, y_tr, X_val3, y_val, y_pred_xgb_tr3, y_pred_xgb_val3, xgb3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logreg = LogisticRegression()\n",
    "# params = {'C': [0.001, 0.01, 0.1, 1, 10], \n",
    "#           'penalty': ['none', 'l1', 'l2', 'elasticnet'],\n",
    "#           'solver': ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga']}\n",
    "# gslog = GridSearchCV(estimator = logreg,\n",
    "#                      param_grid = params,\n",
    "#                      scoring = 'average_precision',\n",
    "#                      cv = 10,\n",
    "#                      n_jobs = -1).fit(X_tr_dum, y_tr)\n",
    "# y_pred_gslog_tr = gslog.predict(X_tr_dum)\n",
    "# y_pred_gslog_val = gslog.predict(X_val_dum)\n",
    "# print(\"Best: %f using %s\" % (gslog.best_score_, gslog.best_params_))\n",
    "# print(\"\")\n",
    "# get_metrics(X_tr_dum, y_tr, X_val_dum, y_val, y_pred_gslog_tr, y_pred_gslog_val, gslog)\n",
    "\n",
    "# Best: 0.532578 using {'C': 0.1, 'penalty': 'l2', 'solver': 'saga'}\n",
    "\n",
    "logb = LogisticRegression(C=1, penalty='l1', solver='saga').fit(X_tr, y_tr)\n",
    "y_pred_logb_tr = logb.predict(X_tr)\n",
    "y_pred_logb_val = logb.predict(X_val)\n",
    "get_metrics(X_tr, y_tr, X_val, y_val, y_pred_logb_tr, y_pred_logb_val, logb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtc = DecisionTreeClassifier()\n",
    "# params = {'criterion': ['gini', 'entropy'],\n",
    "#           'max_depth': [2, 4, 6, 8, 10],\n",
    "#           'min_samples_leaf': [2, 4, 6, 8, 10], \n",
    "#           'min_samples_split': [2, 4, 6, 8, 10]}\n",
    "# gsdtc = GridSearchCV(estimator = dtc,\n",
    "#                      param_grid = params,\n",
    "#                      scoring = 'average_precision',\n",
    "#                      cv = 5,\n",
    "#                      n_jobs = -1).fit(X_tr, y_tr)\n",
    "# y_pred_gsdtc_tr = gsdtc.predict(X_tr)\n",
    "# y_pred_gsdtc_val = gsdtc.predict(X_val)\n",
    "# print(\"Best: %f using %s\" % (gsdtc.best_score_, gsdtc.best_params_))\n",
    "# print(\"\")\n",
    "# get_metrics(X_tr, y_tr, X_val, y_val, y_pred_gsdtc_tr, y_pred_gsdtc_val, gsdtc)\n",
    "\n",
    "# # Best: 0.511668 using {'criterion': 'entropy', 'max_depth': 6, 'min_samples_leaf': 10, 'min_samples_split': 8}\n",
    "\n",
    "dtcb = DecisionTreeClassifier(criterion='entropy', max_depth=6, min_samples_leaf=10, min_samples_split=8).fit(X_tr, y_tr)\n",
    "y_pred_dtcb_tr = dtcb.predict(X_tr)\n",
    "y_pred_dtcb_val = dtcb.predict(X_val)\n",
    "get_metrics(X_tr, y_tr, X_val, y_val, y_pred_dtcb_tr, y_pred_dtcb_val, dtcb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rfc = RandomForestClassifier()\n",
    "# params = {'n_estimators': [100, 200, 400, 600, 1000],\n",
    "#           'criterion': ['entropy', 'gini'],\n",
    "#           'max_depth': [5, 8, 15, 25, 30],\n",
    "#           'min_samples_split': [2, 5, 10, 15, 100],\n",
    "#           'min_samples_leaf': [1, 2, 5, 10]}\n",
    "# gsrfc = GridSearchCV(estimator = rfc,\n",
    "#                      param_grid = params,\n",
    "#                      scoring = 'average_precision',\n",
    "#                      cv = 5,\n",
    "#                      n_jobs = -1).fit(X_tr, y_tr)\n",
    "# y_pred_gsrfc_tr = gsrfc.predict(X_tr)\n",
    "# y_pred_gsrfc_val = gsrfc.predict(X_val)\n",
    "# print(\"Best: %f using %s\" % (gsrfc.best_score_, gsrfc.best_params_))\n",
    "# print(\"\")\n",
    "# get_metrics(X_tr, y_tr, X_val, y_val, y_pred_gsrfc_tr, y_pred_gsrfc_val, gsrfc)\n",
    "\n",
    "# # Best: 0.558041 using {'criterion': 'entropy', 'max_depth': 8, 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 1000}\n",
    "\n",
    "rfcb = RandomForestClassifier(criterion='entropy', max_depth=8, min_samples_leaf=2, min_samples_split=5, n_estimators=1000).fit(X_tr, y_tr)\n",
    "y_pred_rfcb_tr = rfcb.predict(X_tr)\n",
    "y_pred_rfcb_val = rfcb.predict(X_val)\n",
    "get_metrics(X_tr, y_tr, X_val, y_val, y_pred_rfcb_tr, y_pred_rfcb_val, rfcb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abc = AdaBoostClassifier()\n",
    "# params = {'n_estimators': [10, 50, 100, 200],\n",
    "#           'learning_rate': [0.001, 0.01, 0.1, 0.2, 0.5]}\n",
    "# gsabc = GridSearchCV(estimator = abc,\n",
    "#                      param_grid = params,\n",
    "#                      n_jobs = -1,\n",
    "#                      cv = 5,\n",
    "#                      scoring = 'average_precision').fit(X_tr, y_tr)\n",
    "# y_pred_gsabc_tr = gsabc.predict(X_tr)\n",
    "# y_pred_gsabc_val = gsabc.predict(X_val)\n",
    "# print(\"Best: %f using %s\" % (gsabc.best_score_, gsabc.best_params_))\n",
    "# print(\"\")\n",
    "# get_metrics(X_tr, y_tr, X_val, y_val, y_pred_gsabc_tr, y_pred_gsabc_val, gsabc)\n",
    "\n",
    "# Best: 0.542080 using {'learning_rate': 0.2, 'n_estimators': 200}\n",
    "\n",
    "abcb = AdaBoostClassifier(learning_rate=0.2, n_estimators=200).fit(X_tr, y_tr)\n",
    "y_pred_abcb_tr = abcb.predict(X_tr)\n",
    "y_pred_abcb_val = abcb.predict(X_val)\n",
    "get_metrics(X_tr, y_tr, X_val, y_val, y_pred_abcb_tr, y_pred_abcb_val, abcb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gbc = GradientBoostingClassifier()\n",
    "# params = {'n_estimators': [10, 100, 1000],\n",
    "#           'learning_rate': [0.001, 0.01, 0.1],\n",
    "#           'max_depth': [3, 7, 9]}\n",
    "# gsgbc = GridSearchCV(estimator = gbc,\n",
    "#                      param_grid = params, \n",
    "#                      n_jobs = -1, \n",
    "#                      cv = 5, \n",
    "#                      scoring = 'average_precision').fit(X_tr, y_tr)\n",
    "# y_pred_gsgbc_tr = gsgbc.predict(X_tr)\n",
    "# y_pred_gsgbc_val = gsgbc.predict(X_val)\n",
    "# print(\"Best: %f using %s\" % (gsgbc.best_score_, gsgbc.best_params_))\n",
    "# print(\"\")\n",
    "# get_metrics(X_tr, y_tr, X_val, y_val, y_pred_gsgbc_tr, y_pred_gsgbc_tr, gsgbc)\n",
    "\n",
    "# # Best: 0.554906 using {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 1000}\n",
    "\n",
    "gbcb = GradientBoostingClassifier(learning_rate=0.01, max_depth=3, n_estimators=1000).fit(X_tr, y_tr)\n",
    "y_pred_gbcb_tr = gbcb.predict(X_tr)\n",
    "y_pred_gbcb_val = gbcb.predict(X_val)\n",
    "get_metrics(X_tr, y_tr, X_val, y_val, y_pred_gbcb_tr, y_pred_gbcb_val, gbcb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb = XGBClassifier()\n",
    "# params = {'n_estimators': [50, 100, 150, 200], \n",
    "#           'max_depth': [3, 5, 7, 10], \n",
    "#           'min_child_weight': [2, 3, 4, 5]}\n",
    "# gsxgb = GridSearchCV(estimator = xgb,\n",
    "#                      param_grid = params,\n",
    "#                      scoring = 'average_precision',\n",
    "#                      cv = 5,\n",
    "#                      n_jobs = -1).fit(X_tr, y_tr)\n",
    "# y_pred_gsxgb_tr = gsxgb.predict(X_tr)\n",
    "# y_pred_gsxgb_val = gsxgb.predict(X_val)\n",
    "# print(\"Best: %f using %s\" % (gsxgb.best_score_, gsxgb.best_params_))\n",
    "# print(\"\")\n",
    "# get_metrics(X_tr, y_tr, X_val, y_val, y_pred_gsxgb_tr, y_pred_gsxgb_val, gsxgb)\n",
    "\n",
    "# Best: 0.550954 using {'max_depth': 3, 'min_child_weight': 3, 'n_estimators': 50}\n",
    "\n",
    "xgbb = XGBClassifier(max_depth=3, min_child_weight=3, n_estimators=50).fit(X_tr, y_tr)\n",
    "y_pred_xgbb_tr = xgbb.predict(X_tr)\n",
    "y_pred_xgbb_val = xgbb.predict(X_val)\n",
    "get_metrics(X_tr, y_tr, X_val, y_val, y_pred_xgbb_tr, y_pred_xgbb_val, xgbb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Accuracy': [accuracy(y_val, y_pred_logb_val), \n",
    "                     accuracy(y_val, y_pred_dtcb_val), \n",
    "                     accuracy(y_val, y_pred_rfcb_val), \n",
    "                     accuracy(y_val, y_pred_abcb_val), \n",
    "                     accuracy(y_val, y_pred_gbcb_val),\n",
    "                     accuracy(y_val, y_pred_xgbb_val)],\n",
    "        'F1 Score': [f1(y_val, y_pred_logb_val), \n",
    "                     f1(y_val, y_pred_dtcb_val), \n",
    "                     f1(y_val, y_pred_rfcb_val), \n",
    "                     f1(y_val, y_pred_abcb_val), \n",
    "                     f1(y_val, y_pred_gbcb_val),\n",
    "                     f1(y_val, y_pred_xgbb_val)],\n",
    "        'ROC AUC': [auc(X_val, y_val, logb),\n",
    "                    auc(X_val, y_val, dtcb),\n",
    "                    auc(X_val, y_val, rfcb),\n",
    "                    auc(X_val, y_val, abcb),\n",
    "                    auc(X_val, y_val, gbcb),\n",
    "                    auc(X_val, y_val, xgbb)],\n",
    "        'Recall': [recall(y_val, y_pred_logb_val), \n",
    "                   recall(y_val, y_pred_dtcb_val), \n",
    "                   recall(y_val, y_pred_rfcb_val), \n",
    "                   recall(y_val, y_pred_abcb_val),\n",
    "                   recall(y_val, y_pred_gbcb_val),\n",
    "                   recall(y_val, y_pred_xgbb_val)],\n",
    "        'Precision': [precision(y_val, y_pred_logb_val), \n",
    "                      precision(y_val, y_pred_dtcb_val), \n",
    "                      precision(y_val, y_pred_rfcb_val), \n",
    "                      precision(y_val, y_pred_abcb_val),\n",
    "                      precision(y_val, y_pred_gbcb_val),\n",
    "                      precision(y_val, y_pred_xgbb_val)],\n",
    "        'PR AUC': [aps(X_val, y_val, logb),\n",
    "                   aps(X_val, y_val, dtcb),\n",
    "                   aps(X_val, y_val, rfcb),\n",
    "                   aps(X_val, y_val, abcb),\n",
    "                   aps(X_val, y_val, gbcb),\n",
    "                   aps(X_val, y_val, xgbb)]}\n",
    "scores3 = pd.DataFrame(data=data, index = ['Logistic with GridSearchCV', \n",
    "                                          'Random Forest with GridSearchCV', \n",
    "                                          'Decision Tree with GridSearchCV', \n",
    "                                          'AdaBoost with GridSearchCV', \n",
    "                                          'Gradient Boosting with GridSearchCV',\n",
    "                                          'XGBoost with GridSearchCV'])"
   ]
  },
  {
   "source": [
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}